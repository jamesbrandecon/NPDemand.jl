var documenterSearchIndex = {"docs":
[{"location":"priors/#Quasi-Bayes:-Priors-and-Sampling","page":"Quasibayes: Priors and Sampling","title":"Quasi-Bayes: Priors and Sampling","text":"","category":"section"},{"location":"priors/#Quasi-Bayes-at-a-high-level","page":"Quasibayes: Priors and Sampling","title":"Quasi-Bayes at a high level","text":"","category":"section"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"Let l_n(theta) denote a scaled version of our GMM objective function. Additionally, let the transformation e^l_n(theta) denote the quasi-likelihood function and pi(theta) denote a prior density. Then the quasi-posterior takes the form:","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"pi(thetatextdata) = frace^l_n(theta) pi(theta)int e^l_n(theta) pi(theta) dtheta ","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"pi_0(theta^*textdata)propto e^l_n(g^-1(theta^*)) barpi(theta^*)","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"and is a valid density over theta (Chernozhukov and Hong, 2003). ","category":"page"},{"location":"priors/#Priors","page":"Quasibayes: Priors and Sampling","title":"Priors","text":"","category":"section"},{"location":"priors/#Linearly-encoded-priors","page":"Quasibayes: Priors and Sampling","title":"Linearly encoded priors","text":"","category":"section"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"For problems without constraints and with only exchangeability constraints, we specify a simple and dispersed prior:","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"theta sim N(vec 0 I * v)","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"where I is the identity matrix and v is a large prespecified constant. Whenever we impose linear constraints on our parameters (whether or we also impose constraints via SMC), we modify our prior to encode the constraint directly. For example, if our constraints imply that theta_1 (the first element of our parameter vector) must be larger than theta_2, then we introduce an auxiliary parameter theta^*_2 and specify priors on (theta_1 theta_2) as","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"theta_1 theta^*_2 sim N(0v)","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"theta_2 = theta_1 + exp(theta^*_2)","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"We construct similar transformations for all linear constraints. ","category":"page"},{"location":"priors/#More-general-priors","page":"Quasibayes: Priors and Sampling","title":"More general priors","text":"","category":"section"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"More generally, we encode constraints via \"dogmatic priors\" of the form ","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"pi(theta^*) propto barpi(theta^*)mathbf1_mathcalC(theta)","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"where mathcalC denotes the region of parameter space in which all of the desired constraints are satisfied in the data. With this prior, we then have to sample from a quasi-posterior of the form","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"pi(theta^*textdata)propto e^l_n(g^-1(theta^*)) barpi(theta^*)mathbf1_mathcalC(theta)","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"We do this in two steps. First, we sample from the simpler prior described in the previous subsection, and then we run a \"Sequentially Constrained Monte Carlo\" (SMC) algorithm in order to sample from this more complex prior. We describe SMC on its own page in the documentation. ","category":"page"},{"location":"priors/#Sampling","page":"Quasibayes: Priors and Sampling","title":"Sampling","text":"","category":"section"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"By default, for our prior with linear constraints above, the estimate function uses Random Walk Metropolis-Hastings sampling from Turing.jl with proposal step sizes controlled by the step keyword. After importing Turing, NPDemand can also take in custom samplers from Turing.jl, including Hamiltonian Monte Carlo (HMC) and No U-Turn Samplers (NUTS) with auto-differentiation through ForwardDiff.jl. ","category":"page"},{"location":"priors/","page":"Quasibayes: Priors and Sampling","title":"Quasibayes: Priors and Sampling","text":"For SMC (implemented in the smc! function), the user can stil control the step size via step, but we have only implemented Metropolis-Hastings sampling to-date. ","category":"page"},{"location":"functions/#Function-documentation","page":"Function Documentation","title":"Function documentation","text":"","category":"section"},{"location":"functions/#Problem-construction-and-manipulation","page":"Function Documentation","title":"Problem construction and manipulation","text":"","category":"section"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"define_problem","category":"page"},{"location":"functions/#NPDemand.define_problem","page":"Function Documentation","title":"NPDemand.define_problem","text":"define_problem(df::DataFrame; exchange = [], index_vars = [\"prices\"], FE = [], constraints = [], bO = 2, tol = 1e-5)\n\nConstructs a problem::NPDProblem using the provided problem characteristics. Inputs: \n\nexchange: A vector of groups of products which are exchangeable. E.g., with 4 goods, if the first\n\nand second are exchangeable and so are the third and fourth, set exchange = [[1 2], [3 4]].\n\nindex_vars: String array listing column names in df which represent variables that enter the inverted index.\n\"prices\" must be the first element of index_vars\nFE: String array listing column names in df which should be included as fixed effects.\nNote: All fixed effects are estimated as parameters by the minimizer, so be careful adding fixed effects for variables that take many values.\nbO: Order of the univariate Bernstein polynomials in market shares. Default is 2.\nconstraint_tol: Tolerance specifying tightness of constraints\nchunk_size: Controls chunk size in ForwardDiff.Gradient autodiff calculation for nonlinear constraints. Only used if :subs_in_group specified. \nconstraints: A list of symbols of accepted constraints. Currently supported constraints are: \n:monotone  \n:all_substitutes \n:diagonal_dominance_group \n:diagonal_dominance_all \n:subs_in_group (Note: this constraint is the only available nonlinear constraint and will slow down estimation considerably)\nverbose: if false, will not print updates as problem is generated\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"update_constraints!","category":"page"},{"location":"functions/#NPDemand.update_constraints!","page":"Function Documentation","title":"NPDemand.update_constraints!","text":"update_constraints!(problem::NPDProblem, new_constraints::Vector{Symbol})\n\nRe-calculates constraint matrices under new_constraints, ignoring previous constraints used to define the problem. The exchangeability constraint is an exception. To change anything about the structure of exchangeability for the problem changes, define a new problem.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"NPDemand.list_constraints","category":"page"},{"location":"functions/#NPDemand.list_constraints","page":"Function Documentation","title":"NPDemand.list_constraints","text":"list_constraints()\n\nReturns a dictionary of constraints supported by the define_problem function in the NPDemand package. Each constraint is represented by a symbol and its corresponding description.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Estimation","page":"Function Documentation","title":"Estimation","text":"","category":"section"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"estimate!","category":"page"},{"location":"functions/#NPDemand.estimate!","page":"Function Documentation","title":"NPDemand.estimate!","text":"estimate!(problem::NPDProblem;\n    verbose = true,\n    linear_solver = \"Ipopt\", \n    quasi_bayes = false,\n    sampler = [], \n    n_samples::Int = 50_000,\n    burn_in::Real = 0.25, \n    skip::Int = 5,\n    n_attempts = 0,\n    penalty = Inf, \n    step::Union{Real, Symbol} = 0.01)\n\nEstimates the problem using the specified parameters.\n\nArguments\n\nproblem::NPDProblem: The problem to be estimated.\nverbose::Bool: Whether to print verbose output. Default is true.\nlinear_solver::String: The linear solver to use. Must be either \"Ipopt\" or \"OSQP\". Default is \"Ipopt\".\nquasi_bayes::Bool: Whether to use quasi-bayes sampling. Default is false.\nsampler: The sampler to use for quasi-bayes sampling. Default is an empty array.\nn_samples::Int: The number of samples to draw. Default is 50,000.\nburn_in::Real: The fraction of samples to drop as burn-in. Must be less than 1. Default is 0.25.\nskip::Int: The number of samples to skip between saved samples. Default is 5.\nn_attempts: The number of attempts to find a valid starting point for the sampler. Default is 0.\npenalty: The penalty value for the objective function. Default is Inf.\nstep::Union{Real, Symbol}: The step size for the sampler. Can be a real number or the symbol :auto to automatically calculate the step size. Default is 0.01.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"smc!","category":"page"},{"location":"functions/#NPDemand.smc!","page":"Function Documentation","title":"NPDemand.smc!","text":"smc!(problem::NPDemand.NPDProblem;\n    grid_points::Int    = 50, \n    max_penalty::Real   = 5, \n    ess_threshold::Real = 100, \n    step::Real          = 0.1, \n    skip::Int           = 5,\n    burn_in::Real       = 0.25, \n    mh_steps            = max(5, floor(size(problem.results.filtered_chain, 2))/10),\n    seed                = 4132,\n    smc_method          = :adaptive,\n    max_iter            = 1000,\n    adaptive_tolerance  = false, \n    max_violations      = 0.01)\n\nRun sequentially constrained Monte Carlo (SMC) on the problem.\n\nArguments\n\nproblem::NPDemand.NPDProblem: The problem object on which SMC will be run.\n\nOptional Arguments\n\ngrid_points::Int: The number of grid points for the SMC grid. Default is 50.\nmax_penalty::Real: The maximum penalty value for the SMC algorithm. Default is 5.\ness_threshold::Real: The effective sample size threshold for the SMC algorithm. Default is 100.\nstep::Real: The step size for the SMC algorithm. Default is 0.1.\nskip::Int: The thinning factor for the SMC chain. Default is 5.\nburn_in::Real: The fraction of samples to be discarded as burn-in. Default is 0.25.\nmh_steps: The number of Metropolis-Hastings steps per iteration. Default is calculated based on the size of the filtered chain.\nseed: The random seed for the SMC algorithm. Default is 4132.\nsmc_method: The method for choosing the SMC grid. Default is :adaptive.\nmax_iter: The maximum number of iterations for the SMC algorithm. Default is 1000.\nadaptive_tolerance: Whether to use adaptive tolerance for the SMC algorithm. Default is false.\nmax_violations: The maximum allowed fraction of markets with violations. Default is 0.01.\n\nThe function will overwrite the results in the problem object with the resulting chain.\n\nFor harder or slower problems, it may be necessary to increase the number of Metropolis-Hastings steps per iteration (mh_steps), the number of iterations (max_iter), or the maximum allowed fraction markets with violations (max_violations).\n\nburn_in and skip control the number of samples to drop and the thinning of the chain, respectively.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Post-estimation-tools","page":"Function Documentation","title":"Post-estimation tools","text":"","category":"section"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"price_elasticities!","category":"page"},{"location":"functions/#NPDemand.price_elasticities!","page":"Function Documentation","title":"NPDemand.price_elasticities!","text":"price_elasticities!(problem; \n    CI::Union{Vector{Any}, Real} = [], \n    n_draws::Union{Vector{Any}, Int} = [])\n\nTakes the solved problem as first argument, a DataFrame as the second argument, and evaluates all price elasticities in-sample.  Currently does not calculate out-of-sample price elasticities. For this, use the function compute_demand_function!. \n\nResults of this function are stored as a DataFrame in problem.allelasticities. Results can be summarized by hand or using the `summarizeelasticities` function.  We also store the Jacobian of the demand function with respect to prices, which can be used to calculate other quantities of interest.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"elasticity_quantiles","category":"page"},{"location":"functions/#NPDemand.elasticity_quantiles","page":"Function Documentation","title":"NPDemand.elasticity_quantiles","text":"elasticity_quantiles(problem::NPDProblem, ind1::Int, ind2::Int; \n    quantiles = collect(0.01:0.01:0.99),\n    n_draws::Int = 100)\n\nConvenience function for calculating quantiles of price elasticities. problem should be a solved NPDProblem after running price_elasticities!. ind1 and ind2 are the indices of the products for which you want to calculate the quantiles. E.g., ind1=1, ind2=1 returns quantiles of the own-price elasticity for product 1.  The user can control the set of quantiles to return and the number of draws (n_draws) to use in the posterior integration.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"report_constraint_violations","category":"page"},{"location":"functions/#NPDemand.report_constraint_violations","page":"Function Documentation","title":"NPDemand.report_constraint_violations","text":"report_constraint_violations(problem;\n    verbose = true,\n    params = [],\n    output = \"dict\",\n    n_draws::Int = 0)\n\nThis function reports the constraint violations for a given problem.\n\nArguments\n\nproblem: The problem for which constraint violations need to be reported.\nverbose: A boolean indicating whether to print verbose output. Default is true.\nparams: An optional parameter vector. If not provided, the function uses the GMM result if there's no chain, or the filtered chain if available. Default is [].\noutput: The output format for the violations. Default is \"dict\".\nn_draws: The number of draws to use from the filtered chain. Default is 0.\n\nReturns\n\nviolations: A dictionary or a single violation value, depending on the output parameter.\n\nExample output:\n\nDict{Symbol, Float64} with 8 entries:\n  :monotone               => 0.0\n  :all_substitutes        => 0.0\n  :diagonal_dominance_all => 0.0\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"compute_demand_function!","category":"page"},{"location":"functions/#NPDemand.compute_demand_function!","page":"Function Documentation","title":"NPDemand.compute_demand_function!","text":"compute_demand_function!(problem, df; max_iter = 1000, show_trace = false)\n\ncompute_demand_function! estimates the demand function/curve using NPD estimates calculated via estimate!.\n\nThe function takes in an estimated problem::NPDProblem and a dataframe with counterfactual values of the  covariates in the utility function. One must specify all fields that were used in estimation (including shares). The function will change the values of df[!,r\"shares\"] to take on the value of the estimated demand function.\n\nOptions: \n\nmax_iter: controls the number of iterations for the nonlinear solver calculating market shares. Default is 1000 but well-estimated problems should converge faster.\nshow_trace: if true, Optim will print the trace for each iteration of the nonlinear solver. \ncompute_own_elasticities: NOT yet implemented– if true, will also generate columns called own_elast which will include estimated own-price elasticities at all counterfactual points.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"summarize_elasticities","category":"page"},{"location":"functions/#NPDemand.summarize_elasticities","page":"Function Documentation","title":"NPDemand.summarize_elasticities","text":"summarize_elasticities(problem::NPDProblem, which_elasticities::String, stat::String; \n    q = [],\n    integrate = false,\n    n_draws::Int = 100,\n    CI::Real = 0.95)\n\nConvenience function for summarizing price elasticities. problem should be a solved NPDProblem after running price_elasticities!.  stat should be in [\"mean\", \"quantile\"], and if stat==\"quantile\", the option q should include the quantile of interest (e.g., 0.75 for the 75th percentile price elasticities).\n\nWhen a problem has been estimated via quasi-Bayesian methods, the function can integrate over the posterior distribution of the parameters to provide a posterior distribution of the summarized value.  n_draws controls the number of draws from the posterior to use in the integration, and CI controls the with of the credible interval to use in the integration (0.95 for a 95% credible interval).\n\nOutput is a NamedTuple: (;Statistic, PosteriorMean, PosteriorMedian, Posterior_CI)\n\n\n\n\n\n","category":"function"},{"location":"functions/#Back-end-functions","page":"Function Documentation","title":"Back-end functions","text":"","category":"section"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"NPDemand.NPD_parameters","category":"page"},{"location":"functions/#NPDemand.NPD_parameters","page":"Function Documentation","title":"NPDemand.NPD_parameters","text":"NPD_parameters\n\nCustom struct to store estimated parameters specifically. This can be used to replace the candidate parameters in an NPDProblem struct. The two key fields \nare `minimizer` and `filtered_chain`. The `minimizer` field stores the estimated parameters, while the `filtered_chain` field stores the Markov chain for quasi-Bayes\nmethods, after filtering out burn-in and thinning but before reformatting into the full parameter sieve.\n\n\n\n\n\n","category":"type"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"simulate_logit","category":"page"},{"location":"functions/#NPDemand.simulate_logit","page":"Function Documentation","title":"NPDemand.simulate_logit","text":"simulate_logit(J,T, beta, v)\n\nSimulates logit demand for J products in T markets, with price preference parameter beta and market shocks with standard deviation v.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"toDataFrame","category":"page"},{"location":"functions/#NPDemand.toDataFrame","page":"Function Documentation","title":"NPDemand.toDataFrame","text":"toDataFrame(s::Matrix, p::Matrix, z::Matrix, x::Matrix = zeros(size(s)))\n\nConverts the results of simulate_logit to a DataFrame with column names that can be processed by NPDemand.jl\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"bern","category":"page"},{"location":"functions/#NPDemand.bern","page":"Function Documentation","title":"NPDemand.bern","text":"bern(t, order)\n\nReturns a univariate Bernstein polynomial of order order constructed from array/matrix t\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Function Documentation","title":"Function Documentation","text":"dbern","category":"page"},{"location":"functions/#NPDemand.dbern","page":"Function Documentation","title":"NPDemand.dbern","text":"dbern(t, order)\n\nReturns the derivative of a univariate Bernstein polynomial of order order constructed from array/matrix t\n\n\n\n\n\n","category":"function"},{"location":"quasibayes_usage/#Quasi-Bayes-Usage","page":"Using Quasi-Bayes Tools","title":"Quasi-Bayes Usage","text":"","category":"section"},{"location":"quasibayes_usage/","page":"Using Quasi-Bayes Tools","title":"Using Quasi-Bayes Tools","text":"Our implementation of quasi-Bayes (QB) tools includes two main functions: estimate! (the same function used to estimate problems via GMM) and smc!. Some of the underlying details are explained on prior pages. This page focuses on the usage of these functions","category":"page"},{"location":"quasibayes_usage/#Linearly-imposed-constraints","page":"Using Quasi-Bayes Tools","title":"Linearly imposed constraints","text":"","category":"section"},{"location":"quasibayes_usage/","page":"Using Quasi-Bayes Tools","title":"Using Quasi-Bayes Tools","text":"In order to use QB estimate a problem, a standard call to estimate! would look something like the following. ","category":"page"},{"location":"quasibayes_usage/","page":"Using Quasi-Bayes Tools","title":"Using Quasi-Bayes Tools","text":"using Turing\n\nestimate!(problem, \n        quasi_bayes = true,\n        burn_in = 0.50,     # Fraction of samples to discard as burn-in\n        n_attempts = 100,   # Number of samples used to identify a valid starting point \n        n_samples = 15000,  # Total (including burn-in) samples to collect\n        step = 0.01,        # Controls MH step size\n        skip = 5,           # Controls chain thinning. E.g., skip=5 stores only every fifth sample\n        sampler = MH())     # control the sampler used by Turing.jl","category":"page"},{"location":"quasibayes_usage/","page":"Using Quasi-Bayes Tools","title":"Using Quasi-Bayes Tools","text":"One way to speed up sampling is to use Hamiltonian Monte Carlo (HMC()) or No U-Turn Sampler (NUTS()). Another is to increase the chunksize used by the autodiff tools inside of Turing, as below:","category":"page"},{"location":"quasibayes_usage/","page":"Using Quasi-Bayes Tools","title":"Using Quasi-Bayes Tools","text":"HMC(0.01,\n     5; \n     adtype = Turing.AutoForwardDiff(chunksize = 1000))","category":"page"},{"location":"quasibayes_usage/","page":"Using Quasi-Bayes Tools","title":"Using Quasi-Bayes Tools","text":"warning: Warning\nAlthough AutoZygote is an often-recommended alternative to AutoForwardDiff, it will not work in the current package due to choices we've made internally. ","category":"page"},{"location":"quasibayes_usage/","page":"Using Quasi-Bayes Tools","title":"Using Quasi-Bayes Tools","text":"After this has been run, the problem stores two key objects. One is the original chain (without any burn-in or thinning) in problem.chain, and the other is a \"filtered\" chain (with burn-in and thinning) in problem.results.filtered_chain. Post-estimation tools will use the filtered chain for calculating price elasticities and counterfactuals. ","category":"page"},{"location":"quasibayes_usage/#Nonlinearly-imposed-constraints-(SMC)","page":"Using Quasi-Bayes Tools","title":"Nonlinearly imposed constraints (SMC)","text":"","category":"section"},{"location":"quasibayes_usage/","page":"Using Quasi-Bayes Tools","title":"Using Quasi-Bayes Tools","text":"In order to use SMC, you must estimate! a problem first, with quasibayes set to true. Then, you can use the following command: ","category":"page"},{"location":"quasibayes_usage/","page":"Using Quasi-Bayes Tools","title":"Using Quasi-Bayes Tools","text":" smc!(problem, \n        burn_in = 0.50, \n        skip = 20,\n        max_penalty = 6,            # Maximum allowed penalty term\n        step = 0.01, \n        mh_steps = 5,               # Number of Metropolis-Hastings steps every iteration\n        seed = 1021, \n        ess_threshold = 200,        # Minimum acceptable Effective Sample Size per iteration\n        smc_method = :adaptive,     # Calculate the penalty sequence adaptively\n        max_violations = 0.05,      # Maximum acceptable fraction of markets with any violations of constraints\n        max_iter = 100)             # Maximum number of iterations ","category":"page"},{"location":"constraints/#Constraints","page":"Constraints and SMC","title":"Constraints","text":"","category":"section"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"Below is a list of constraints which have been implemented in NPDemand so far. We impose these constraints on our nonparametric demand functions in two ways: one which is linear in parameters, and another which requires quasi-Bayes sampling over our objective function. We discuss both approaches below. ","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"Constraint Symbol Linear constraint\nNegative own-price elasticities :monotone Yes\nDiagonal Dominance within exchangeable groups :diagonal_dominance_in_group Yes\nDiagonal Dominance among all products :diagonal_dominance_all Yes\nAll products substitutes :all_substitutes Yes\nProducts in the same group are substitutes :subs_in_group No\nProducts in different groups are substitutes :subs_across_group No\nProducts in the same group are substitutes :complements_in_group No\nProducts in different groups are substitutes :complements_across_group No","category":"page"},{"location":"constraints/#Linear-constraints","page":"Constraints and SMC","title":"Linear constraints","text":"","category":"section"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"Our first constraint implementation is a linear constraints as developed by Compiani (2022). Each of these takes advantage of the fact that coefficients of Bernstein polynomials approximate values of the target function (the inverse demand function) on a known grid. As a result, we can easily impose some constraints, like monotonicity on the inverse demand function, by imposing linear relationships between sieve parameters. Constraints which fall into this category are denoted above in the Linear column","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"It is important to note that these linear constraints on our sieve parameters are, in general, necessary for the desired economic constraints to be satisfied by the resulting demand function, but not sufficient. However, they are much easier to impose than the more complex constraints we discuss below. In practice, we find that these linear constraints are often sufficient to provide reasonable orders of magnitude and relationships between products at the median, but insufficient for other moments of the elasticity distribution to be well-behaved. For more information, see discussions in Compiani (2022) and Brand and Smith (2024).","category":"page"},{"location":"constraints/#Quasi-bayes-constraints-and-Sequentially-Constrained-Monte-Carlo-(SMC)","page":"Constraints and SMC","title":"Quasi-bayes constraints and Sequentially Constrained Monte Carlo (SMC)","text":"","category":"section"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"<!– #### Dogmatic Priors The first way we allow users to impose constraints using our quasi-Bayes approach is via a penalty term (the penalty argument in estimate!). The default value of this penalty is 0, but when the user increases this value, we evaluate a modified objective function. Rather than sampling according to the quasi-likelihood, we penalize that likelihood by the provided penalty:","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"# Turing.@addlogprob! (-0.5 * objective(...) - penalty)","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"We find that this approach has an imperfect success rate. If multiple Markov chains are constructed for the same problem via this method, some may find the constrained space quickly and remain there, and others will never find the space even after many tens of thousands of samples. Still, we include this option because of its ease of use and its speed relative to our final approach below.  –>","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"<!– #### Sequentially Constrained Monte Carlo (SMC) –>  Our more complex option for imposing constraints is implemented in the smc! function. This function implements \"Sequentially Constrained Monte Carlo,\" an iterative method which slowly reshapes an unconstrained Markov chain into a potentially complex constrained space. The approach is intutively simple. We specify an increasing sequence of penalty terms lambda_i, which (after smoothing) define a sequence of posteriors pi_i from which we can sample (with likelihood l_n and unconstrained posterior barpi): ","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"pi_0(theta^*textdata)propto e^l_n(g^-1(theta^*)) barpi(theta^*)","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"pi_1(theta^*textdata)propto e^l_n(g^-1(theta^*)) barpi(theta^*) lVertthetarVert_mathcalC^lambda_1","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"pi_2(theta^*textdata)propto e^l_n(g^-1(theta^*)) barpi(theta^*) lVertthetarVert_mathcalC^lambda_2","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"$","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"\\vdots $  pi_M(theta^*textdata)propto e^l_n(g^-1(theta^*)) barpi(theta^*) lVertthetarVert_mathcalC^lambda_M","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"We smooth the lambda penalty terms via the following, where Phi is the standard normal CDF:  theta_mathcalC^lambda = prod_t=1^T Phi(-lambda c_t(theta))","category":"page"},{"location":"constraints/","page":"Constraints and SMC","title":"Constraints and SMC","text":"We define this sequence adaptively by default, though we offer users other options in the package. At each step in the sequence, we take a relatively small number of Metropolis-Hastings steps for each element of our chain (set by the user via mh_steps), sampling from the quasi-posterior corresponding to the next element lambda_i+1. As we demonstrate in Brand and Smith (2024), appropriately defining this sequence allows us to enforce the desired constraints globally on our data. The corresponding cost is that this method is by far the most time consuming to run, often taking many minutes or even hours, depending on the problem's complexity. ","category":"page"},{"location":"details/#Implementation-Details","page":"Setup and GMM","title":"Implementation Details","text":"","category":"section"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"This page explains both the underlying economic model that our package assumes and some of the details concerning our approach, which implements a nonparametric estimator as in Compiani, 2022 with a more general suite of (linear and nonlinear) constraints and implementation/estimation options. ","category":"page"},{"location":"details/#The-Nonparametric-Demand-Estimation-Problem","page":"Setup and GMM","title":"The Nonparametric Demand Estimation Problem","text":"","category":"section"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"Our package estimates a demand function for J goods, allowing flexible substitution patterns including some forms of complementarities. Letting s denote market shares, the assumed demand function takes the following form: ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"    s_jt = sigma_j(delta_jt)","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"where jt index products and markets respectively. Note that sigma is indexed by j, allowing the demand functions of different goods to take different forms. The arguments of sigma are delta, which we assume takes the following form: ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"    delta_jt = beta x_jt + xi_jt","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"where x_jt denote observable product characteristics including price, and xi_jt denotes unobservable demand shifters at the product-market level. Direct estimation of sigma would be complicated here due to the fact that xi enters sigma nonlinearly. Instead, under conditions stated in Compiani, 2022, we can invert sigma and estiamte the resulting inverse. The inverse demand equation is then: ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"    x_jt = sigma^-1_j(s_jt) - xi_jt","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"The inverse demand functions can then be estimated via Nonparametric Instrumental Variables (NPIV). We discuss the exact implementation below. ","category":"page"},{"location":"details/#Bernstein-Polynomials","page":"Setup and GMM","title":"Bernstein Polynomials","text":"","category":"section"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"One key decision in estimating the functions sigma^-1_j is how to approximate them. In this package we approximate each function using Bernstein polynomials. A Bernstein polynomial of order n takes the form of a sum of basis functions:  ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"B_n(x) = sum_k=1^n theta_k b_kn(x)","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"where the basis functions are defined as:","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"b_kn(x) =  n choose k x^k (1-x)^n-k","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"Although one could make other reasonable choices for approximating sigma^-1_j, Bernstein polynomials were chosen because they an approximation for which it is uniquely easy to impose constraints, which is a central focus of our package. ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"Whenever you run define_problem, we generate two Bernstein polynomials. The first is in market shares, and serves as the design matrix for the NPIV regression. The second is in the vector of instruments. ","category":"page"},{"location":"details/#GMM-Estimation","page":"Setup and GMM","title":"GMM Estimation","text":"","category":"section"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"The estimation problem takes the following form: ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"    min_theta sum_j=1^J Big sum_t=1^T tilde xi_jt Big (A_jA_j)^- Big sum_t=1^T tilde xi_jt Big ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"where tilde xi_jt are the values of xi_jt implies by the current estimate of the demand system. The matrix A_j is the aforementioned Bernstein polynomial in demand instruments for product j.","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"When we are solving this linear problem using JuMP.jl, we solve it as written. When we are using more general approaches (Optim.jl or Turing.jl), we found this problem to be difficult to efficiently solve as written. Instead, during the construction of the problem, we multiply out the matrices above, allowing us to store and manipulate much smaller matrices for evaluating our objective functions. In particular, for appropriately defined values of yX and Z, we can rewrite the objective function as ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"    min_theta (y - Xtheta) Z (y - X theta)","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"For complicated problems, theta can include hundreds of parameters, meaning that Z and X may have hundreds of columns (and, in the case of Z, rows). Manipulating  these large matrices directly resulted in an estimation process that was quite slow even with analytic gradients. However, note that if we multiply out the product above, we get ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"    yZy + yZ Xtheta - thetaXZy + thetaXX theta ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"Note, then, that the first term is constant with respect to theta, and the rest of the terms are relatively small matrices which can be pre-computed prior to estimation.  In practice, to handle normalizations and to treat the components theta that correspond to the index delta differently from those that correspond to the inverse demand functions themselves,  we use a slightly different formulation, but the impact is the same. By pre-computing these matrices and storing them during problem construction, we find that estimation is dramatically faster.  The only cost we pay is that this process introduces a small amount of (floating-point) errors in our objective function construction, but we find that these are negligible even in simulated data with unreasonably high signal to noise ratios (i.e., small optimized objective values).  ","category":"page"},{"location":"details/#Fixed-Effects","page":"Setup and GMM","title":"Fixed Effects","text":"","category":"section"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"Fixed effects are estimated as parameters, not absorbed from the data. So, be careful including fixed effects with too many values, as this may both slow down the optimization and require more memory.","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"To include fixed effects (categorical variables), use the option FE to provide a vector of strings, where each element of the vector is a name of a column in the provided data df. Note however that at present, variables that are included as fixed effect must be constant across products within a market. The only exception to this rule is \"product\" which is a keyword which will produce product fixed-effects. There need not be a column named product in the data, and in fact the code will ignore it if it's there. ","category":"page"},{"location":"details/","page":"Setup and GMM","title":"Setup and GMM","text":"warning: Warning\nGiven how commmon fixed effect regression packages have become, users may expect the package to smartly identify and drop singletons and perform colinearity checks. This it not yet implemented– at present, the only thing we do beyond including all fixed effects as dummy variables is to drop one level per dimension of fixed effects. ","category":"page"},{"location":"postestimation/#Post-Estimation","page":"Post-Estimation","title":"Post-Estimation","text":"","category":"section"},{"location":"postestimation/#Estimated-Price-Elasticities","page":"Post-Estimation","title":"Estimated Price Elasticities","text":"","category":"section"},{"location":"postestimation/","page":"Post-Estimation","title":"Post-Estimation","text":"The main output that can be calculated from the results of estimate! is the full set of price elasticities of demand. Calculating all price elasticities requires  a single call to the price_elasticities! function. After this call, you can access the full set of elasticities via the field npd_problem.all_elasticities, which is an Array of matrices.  Each element of the array is the matrix of price elasticities for a market, where the (ij) element of each matrix denotes the elasticity of demand for product i with respect to the price of product j. Alternatively, you can extract the own-price elasticities for each product using the own_elasticities function or summarize elasticities across markets using the summarize_elasticities function. Some example usage: ","category":"page"},{"location":"postestimation/","page":"Post-Estimation","title":"Post-Estimation","text":"price_elasticities!(npd_problem); # Calculate all elasticities \n\nown = own_elasticities(npd_problem) # Extract own-price elasticities \n\n# Summaries of elasticities\nsummarize_elasticities(npd_problem, \"median\")\nsummarize_elasticities(npd_problem, \"mean\")\nsummarize_elasticities(npd_problem, \"quantile\"; q = 0.3) # 30th percentile","category":"page"},{"location":"postestimation/#Estimated-Demand-Functions","page":"Post-Estimation","title":"Estimated Demand Functions","text":"","category":"section"},{"location":"postestimation/","page":"Post-Estimation","title":"Post-Estimation","text":"One of the benefits of nonparametric demand estimation is that it can allow us to study the shape of the demand function and compare it to the demand functions implied by alternative models. For this purpose, or for evaluating the impact of counterfactual prices, it may be useful to estimate the demand function for a given set of prices of one good while keeping prices of other goods fixed, or at a vector of alternative prices. ","category":"page"},{"location":"postestimation/","page":"Post-Estimation","title":"Post-Estimation","text":"This can be done using the compute_demand_function! function, which takes in a NPDProblem and a DataFrame which contains prices and market shares. The function then modifies the provided DataFrame, filling in the columns shares0, shares1, ..., sharesJ with the estimated shares for each market. This is a sightly more complicated task than in many structural models, because the parameters we are estimating govern the inverse demand system rather than the demand system itself. In order to calculate the (non-inverted) demand function, we have to perform a nonlinear search over market shares to match the user-provided prices and other index covariates. ","category":"page"},{"location":"postestimation/","page":"Post-Estimation","title":"Post-Estimation","text":"warning: Warning\ncompute_demand_function! can currently only calculate counterfacual market shares assuming that all demand shocks (xi) are set to zero.  ","category":"page"},{"location":"postestimation/","page":"Post-Estimation","title":"Post-Estimation","text":"Here is an example of how to use this function to compute the demand function, varying the price of product 1 while keeping prices of other goods fixed. ","category":"page"},{"location":"postestimation/","page":"Post-Estimation","title":"Post-Estimation","text":"# Create a new DataFrame which can be filled in with estimated demand functions\nalt_price_df = DataFrame(); # Initialize dataframe \nalt_price_df.prices1 = 1.1 .* ones(10); alt_price_df.prices2 .= 1.1; alt_price_df.prices3 .= 1.1;\nalt_price_df.prices0 .= collect(range(0.7,1.3, length=10));\nfor j = 0:3\n    alt_price_df[!,\"shares$j\"] .=0; # shares set to zero to initialize the fields-- compute_demand_function will ignore and replace these values\n    alt_price_df[!,\"x$j\"] .= 0.5; \nend\n\ncompute_demand_function!(npd_problem, alt_price_df; max_iter = 1000, show_trace = false);","category":"page"},{"location":"postestimation/","page":"Post-Estimation","title":"Post-Estimation","text":"As mentioned in the note above, if you run this code you will notice that the output above generates a warning that the function \"Assumes residual demand shifters set to zero.\" This refers to the values of the residuals xi. Other alternative values for xi will likely be implemented in future versions of the package. After running compute_demand_function!, we can then plot the estimated demand functions (stored in shares0...shares3). Here is some example code which plots all counterfactual shares: ","category":"page"},{"location":"postestimation/","page":"Post-Estimation","title":"Post-Estimation","text":"plot(p, alt_price_df.prices0, alt_price_df.shares0, \n    color = :red, \n    lw = 1.5, label = \"Est. Share 1\", \n    xlabel = \"Price 1\", \n    ylabel = \"Market Share\");\nplot!(alt_price_df.prices0, alt_price_df.shares1, color = :grey, label = \"Est. Share 2\");\nplot!(alt_price_df.prices0, alt_price_df.shares2, color = :grey, label = \"Est. Share 3\");\nplot!(alt_price_df.prices0, alt_price_df.shares3, color = :grey, label = \"Est. Share 4\");","category":"page"},{"location":"usage/#Basic-Usage","page":"Basic Usage (GMM)","title":"Basic Usage","text":"","category":"section"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"Having described the some of the estimation details, we now describe how to use the package.","category":"page"},{"location":"usage/#Defining-Problems","page":"Basic Usage (GMM)","title":"Defining Problems","text":"","category":"section"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"The first step to estimating demand is to define the problem. The define_problem function takes the following arguments:","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"df: a DataFrame containing the data to be used in estimation.\nexchange: a list of lists of integers, where each list of integers corresponds to a group of products that are exchangeable.\nindex_vars: a list of strings, where each string corresponds to a column in df that contains the index variables for the demand system. \nconstraints: a list of symbols, where each symbol corresponds to a constraint to be imposed on the demand system.\nbO: an integer indicating the order of Bernstein polynomials to be used in the demand system. The default value is 2, and larger values will result in (significantly) more parameters.\nFE: a list of strings, where each string corresponds to a column in df that contains fixed effects to be included in the demand system.\nconstraint_tol: the tolerance for the constraint satisfaction problem.","category":"page"},{"location":"usage/#Constraints","page":"Basic Usage (GMM)","title":"Constraints","text":"","category":"section"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"So far, we have implemented the following constraints: ","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":":exchangeability: Products given by exchange are exchangeable, meaning that product identities do not affect demand. Product fixed effects can stil be estimated\n:monotone: All demand functions are monotonic in their own index \n:diagonal\\_dominance\\_all: All demand functions are diagonally dominant\n:diagonal\\_dominance\\_group: All demand functions are diagonally dominant within exchangeable groups\n:all_substitutes: All inverse demand functions are increasing in all indexes, which is a necessary condition for all products to be substitutes\n:subs\\_in\\_group: Cross-price elasticities of demand are positive for every pair of goods within an exchangeable group. No restrictions on the Jacobian are made across groups","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"All of these constraints are linear except for the last. ","category":"page"},{"location":"usage/#Example","page":"Basic Usage (GMM)","title":"Example","text":"","category":"section"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"Here is an example of a problem definition using many of the options described above: ","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"bO = 2; \nexchange = [[1 2], [3 4]] # the first and second products are exchangeable, as are the third and fourth\nindex_vars = [\"prices\", \"x\"]\n\nconstraints = [:exchangeability, :diagonal_dominance_all, :monotone]; \nnpd_problem = define_problem(df;  \n                            exchange = exchange,\n                            index_vars = index_vars, \n                            constraints = constraints,\n                            bO = bO,\n                            FE = []); ","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"After running this command, npd_problem is now of type NPDemand.NPDProblem. The problem contains many components that would be difficult for users to read and understand, so for ease of use we include a show method for NPDemand.NPDProblem which prints out the core components of the problem definition. ","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"@show npd_problem;","category":"page"},{"location":"usage/#One-line-estimation:-estimate!","page":"Basic Usage (GMM)","title":"One-line estimation: estimate!","text":"","category":"section"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"Estimation of a defined problem can be done via a call to the NPDemand.estimate! function: ","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"estimate!(npd_problem)","category":"page"},{"location":"usage/#Price-elasticities","page":"Basic Usage (GMM)","title":"Price elasticities","text":"","category":"section"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"Similarly, we can then calculate all price elasticities between products in all markets via","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"price_elasticities!(problem)","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"We can also quickly summarize the median of each element of the elasticity matrix: ","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"summarize_elasticities(problem, \"matrix\", \"quantile\", q = 0.5)","category":"page"},{"location":"usage/","page":"Basic Usage (GMM)","title":"Basic Usage (GMM)","text":"More details are included in the Advanced Usage and Functions pages. ","category":"page"},{"location":"#NPDemand.jl","page":"NPDemand.jl","title":"NPDemand.jl","text":"","category":"section"},{"location":"","page":"NPDemand.jl","title":"NPDemand.jl","text":"CurrentModule = NPDemand","category":"page"},{"location":"","page":"NPDemand.jl","title":"NPDemand.jl","text":"Documentation for NPDemand.jl, a package for nonparametric demand estimation. This version of the package will be introduced formally and used by (in-progress) Brand and Smith (2025).  ","category":"page"},{"location":"#Package-Framework","page":"NPDemand.jl","title":"Package Framework","text":"","category":"section"},{"location":"","page":"NPDemand.jl","title":"NPDemand.jl","text":"This package is meant to make the estimation of price elasticities as easy, and as close to reg y x, r, as possible. There are three necessary steps: ","category":"page"},{"location":"","page":"NPDemand.jl","title":"NPDemand.jl","text":"Define a problem (an NPDProblem): at this stage, you specify your data, your model, and any econometric choices that are required up front. \nEstimate the problem: in the simplest case, this is a single call to the estimate! function. Otherwise, specify details about how long to let the estimation run and other similar control options.\nProcess results: the problem has now stored the estimated parameters internally, and we've tried to provide functions to calculate the main objects of interest (price elasticities and demand functions) that do not require the user to know anything about what is happening under the hood. ","category":"page"},{"location":"","page":"NPDemand.jl","title":"NPDemand.jl","text":"This is similar to, and inspired by, the structure of the (much more extensive) PyBLP package in Python. Ideally, this structure makes the code modular enough that our package could be appended (either by the user or by us) with additional functions for more complicated processing and other results. ","category":"page"},{"location":"#Documentation-Contents","page":"NPDemand.jl","title":"Documentation Contents","text":"","category":"section"},{"location":"","page":"NPDemand.jl","title":"NPDemand.jl","text":"","category":"page"}]
}
